{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jerry-at-GH/fansub-utils/blob/main/auto_timing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRGXxH2HR6Ju"
      },
      "source": [
        "# 自动打轴\n",
        "\n",
        "- 输入的译稿需要符合如下格式（如无需中文，`\\N`后留空即可）：\n",
        "    ```\n",
        "    日日日日？\\N中中？\n",
        "    日日日\\N中中中（注释以左括号开头\n",
        "    （注释以左括号开头\n",
        "    日日 日日日\\N中中 中中中\n",
        "    ```\n",
        "- 运行结束后，会自动下载输出文件 `fa+vad.ass`\n",
        "- 设计目标是尽量贴紧实际说话时间，未做前后留白"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBwABEDHUjbJ"
      },
      "outputs": [],
      "source": [
        "# @title  {\"run\":\"auto\",\"display-mode\":\"form\"}\n",
        "VIDEO_PATH = \"\" # @param {\"type\":\"string\",\"placeholder\":\"如果文件在 Drive 内：/content/drive/MyDrive/...\"}\n",
        "TRANSCRIPT_PATH = \"\" # @param {\"type\":\"string\",\"placeholder\":\"如果文件在 Drive 内：/content/drive/MyDrive/...\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_D9PXJakAZA"
      },
      "source": [
        "## 请运行下列所有代码"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b21WkFmcejZj"
      },
      "source": [
        "### Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LiMS2W1oSWQX"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jhpl3VLkgUE6"
      },
      "outputs": [],
      "source": [
        "!apt update\n",
        "!apt install libc++1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9R0hftgnSt62"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/NVIDIA/NeMo --depth=1\n",
        "!uv pip install pysubs2 \"nemo_toolkit[asr]\" \"audio-separator[gpu]\"\n",
        "!uv pip install -U --force-reinstall -v git+https://github.com/TEN-framework/ten-vad.git\n",
        "!uv pip install \"numpy<2.0.0\" scipy --force-reinstall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ut_G9PRuagbJ"
      },
      "outputs": [],
      "source": [
        "!ffmpeg -i $VIDEO_PATH -ac 1 -ar 16000 mono_16k.wav"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mc52ZpjPbCEj"
      },
      "source": [
        "### VAD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMbjwLwyU_hO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pysubs2\n",
        "from scipy.signal import resample\n",
        "\n",
        "\n",
        "AUDIO = \"mono_16k.wav\"\n",
        "VOCALS_AUDIO = \"vocals_mono_16k.wav\"\n",
        "WORK_DIR = \"vad\"\n",
        "FRAME_HOP_SEC = 0.01\n",
        "!rm -rf $WORK_DIR\n",
        "!mkdir $WORK_DIR\n",
        "\n",
        "\n",
        "def zeroify_period(signal, start_sec, end_sec):\n",
        "    start_idx = int(start_sec / FRAME_HOP_SEC) if start_sec >= 0 else int((len(signal) + start_sec / FRAME_HOP_SEC))\n",
        "    end_idx = int(end_sec / FRAME_HOP_SEC) if end_sec >= 0 else int((len(signal) + end_sec / FRAME_HOP_SEC))\n",
        "    signal[start_idx : end_idx + 1] = 0\n",
        "\n",
        "\n",
        "def bias_toward_half(x, k=0.4):\n",
        "    return (x**k) / (x**k + (1 - x) ** k)\n",
        "\n",
        "\n",
        "def pred_to_vad(pred, onset, offset, min_on_sec=0.0, min_off_sec=0.0):\n",
        "    onset, offset = onset, offset\n",
        "    min_on_frames = round(min_on_sec / FRAME_HOP_SEC)\n",
        "    min_off_frames = round(min_off_sec / FRAME_HOP_SEC)\n",
        "\n",
        "    # hysteresis threshold\n",
        "    vad_labels = np.zeros_like(pred, dtype=bool)\n",
        "    in_speech = False\n",
        "    for i, val in enumerate(pred):\n",
        "        if not in_speech:\n",
        "            if val >= onset:\n",
        "                in_speech = True\n",
        "                vad_labels[i] = True\n",
        "        else:\n",
        "            if val < offset:\n",
        "                in_speech = False\n",
        "            else:\n",
        "                vad_labels[i] = True\n",
        "\n",
        "    # utility to get contiguous (start, end) from boolean array\n",
        "    def get_segments(labels):\n",
        "        segs = []\n",
        "        start = None\n",
        "        for i, val in enumerate(labels):\n",
        "            if val and start is None:\n",
        "                start = i\n",
        "            elif not val and start is not None:\n",
        "                segs.append((start, i - 1))\n",
        "                start = None\n",
        "        if start is not None:\n",
        "            segs.append((start, len(labels) - 1))\n",
        "        return segs\n",
        "\n",
        "    # remove short speech segments\n",
        "    segments = get_segments(vad_labels)\n",
        "    filtered = []\n",
        "    for start, end in segments:\n",
        "        if end - start >= min_on_frames:\n",
        "            filtered.append((start, end))\n",
        "\n",
        "    # merge short non-speech gaps\n",
        "    merged = []\n",
        "    for seg in filtered:\n",
        "        if not merged:\n",
        "            merged.append(seg)\n",
        "        else:\n",
        "            prev_start, prev_end = merged[-1]\n",
        "            if seg[0] - prev_end < min_off_frames:\n",
        "                merged[-1] = (prev_start, seg[1])  # merge\n",
        "            else:\n",
        "                merged.append(seg)\n",
        "\n",
        "    vad = np.zeros_like(vad_labels, dtype=bool)\n",
        "    for s, e in merged:\n",
        "        vad[s : e + 1] = True\n",
        "\n",
        "    return merged, vad\n",
        "\n",
        "\n",
        "def to_srt(merged, srt_file):\n",
        "    subs = pysubs2.SSAFile()\n",
        "    for start, end in merged:\n",
        "        subs.append(pysubs2.SSAEvent(start=start * 1000 * FRAME_HOP_SEC, end=end * 1000 * FRAME_HOP_SEC, text=\"???\"))\n",
        "    subs.save(srt_file)\n",
        "\n",
        "\n",
        "# ten, 10ms/frame\n",
        "from ten_vad import TenVad\n",
        "import scipy.io.wavfile as Wavfile\n",
        "\n",
        "sr, data = Wavfile.read(AUDIO)\n",
        "hop_size = 160  # 16000Hz/(1s/10ms)\n",
        "threshold = 0.5\n",
        "ten_vad_instance = TenVad(hop_size, threshold)\n",
        "num_frames = data.shape[0] // hop_size\n",
        "ten = np.array([ten_vad_instance.process(data[i * hop_size : (i + 1) * hop_size])[0] for i in range(num_frames)])[1:]\n",
        "# np.save(f\"{WORK_DIR}/ten.npy\", ten)\n",
        "\n",
        "\n",
        "# heuristic\n",
        "from audio_separator.separator import Separator\n",
        "import librosa\n",
        "import shutil\n",
        "\n",
        "\n",
        "separator = Separator()\n",
        "separator.load_model(model_filename=\"mel_band_roformer_vocals_fv4_gabox.ckpt\")\n",
        "output_files = separator.separate(globals().get('VIDEO_PATH', None), {\"Vocals\": \"vocals\", \"Instrumental\": \"inst\"})\n",
        "shutil.move(\"vocals.wav\", f\"{WORK_DIR}/vocals.wav\")\n",
        "shutil.move(\"inst.wav\", f\"{WORK_DIR}/inst.wav\")\n",
        "!ffmpeg -i $WORK_DIR/vocals.wav -ac 1 -ar 16000 $WORK_DIR/vocals_mono_16k.wav\n",
        "\n",
        "y, sr = librosa.load(f\"{WORK_DIR}/{VOCALS_AUDIO}\")\n",
        "\n",
        "rms = librosa.feature.rms(y=y)[0]\n",
        "rms_downsampled = np.clip(resample(rms, len(ten)), 0, rms.max())\n",
        "\n",
        "rolloff = librosa.feature.spectral_rolloff(y=y + 0.1, sr=sr)[0]\n",
        "rolloff_downsampled = np.clip(resample(rolloff, len(ten)), 0, rolloff.max())\n",
        "\n",
        "heuristic = rms_downsampled * rolloff_downsampled\n",
        "heuristic = (heuristic - heuristic.min()) / (heuristic.max() - heuristic.min())\n",
        "# np.save(f\"{WORK_DIR}/heuristic.npy\", heuristic)\n",
        "\n",
        "\n",
        "# ensemble\n",
        "pred = np.mean(\n",
        "    np.stack(\n",
        "        [\n",
        "            ten,\n",
        "            bias_toward_half(np.clip((heuristic - 0.005), 0, 0.1) / 0.1, 3),\n",
        "        ],\n",
        "        axis=1,\n",
        "    ),\n",
        "    axis=1,\n",
        ")\n",
        "pred[pred < 0] = 0\n",
        "merged, vad = pred_to_vad(pred, 0.4, 0.5, min_on_sec=0.03)\n",
        "to_srt(merged, \"vad.srt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgnaZ9udbwJh"
      },
      "source": [
        "### Forced alignment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ovdcx78zby_D"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "import pysubs2\n",
        "\n",
        "\n",
        "AUDIO = r\"mono_16k.wav\"\n",
        "WORK_DIR = \"nfa\"\n",
        "!rm -rf $WORK_DIR\n",
        "!mkdir $WORK_DIR\n",
        "\n",
        "\n",
        "with open(TRANSCRIPT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.readlines()\n",
        "    lines = [l.strip() for l in lines]\n",
        "WORD_CHARS = \"A-Za-z0-9\\u3040-\\u309f\\u30a0-\\u30ff\\u3400-\\u4dbf\\u4e00-\\u9fff\\uf900-\\ufaff\"\n",
        "lines = [\n",
        "    {\n",
        "        \"raw\": l.strip(),\n",
        "        \"processed\": re.sub(rf\"[^{WORD_CHARS}]+\", \" \", l.split(r\"\\N\")[0]).strip()\n",
        "        if \"(\" not in l and \"（\" not in l and l.strip() not in [\"OP\", \"ED\"]\n",
        "        else \"\",\n",
        "    }\n",
        "    for l in lines\n",
        "    if len(l.strip()) > 0\n",
        "]\n",
        "lines = [{**l, \"match\": re.sub(rf\"[^{WORD_CHARS}]+\", \"\", l[\"processed\"])} for l in lines]\n",
        "\n",
        "\n",
        "manifest_filepath = f\"{WORK_DIR}/manifest.json\"\n",
        "manifest_data = {\n",
        "    \"audio_filepath\": AUDIO,\n",
        "    \"text\": \"|\".join([l[\"processed\"] for l in lines if l[\"processed\"]]),\n",
        "}\n",
        "with open(manifest_filepath, \"w\") as f:\n",
        "    line = json.dumps(manifest_data)\n",
        "    f.write(line + \"\\n\")\n",
        "\n",
        "\n",
        "!python /content/NeMo/tools/nemo_forced_aligner/align.py \\\n",
        "  pretrained_name=\"nvidia/parakeet-tdt_ctc-0.6b-ja\" \\\n",
        "  manifest_filepath=$manifest_filepath \\\n",
        "  output_dir=$WORK_DIR/nfa_output/ \\\n",
        "  additional_segment_grouping_separator=\"|\" \\\n",
        "  ass_file_config.vertical_alignment=\"bottom\" \\\n",
        "  ass_file_config.text_already_spoken_rgb=\"[66,245,212]\" \\\n",
        "  ass_file_config.text_being_spoken_rgb=\"[242,222,44]\" \\\n",
        "  ass_file_config.text_not_yet_spoken_rgb=\"[223,242,239]\"\n",
        "\n",
        "\n",
        "ctm_filepath = f\"{WORK_DIR}/nfa_output/ctm/tokens/{AUDIO.split('/')[-1].split('.')[0]}.ctm\"\n",
        "with open(ctm_filepath, \"r\") as f:\n",
        "    ctm_lines = f.read().split(\"\\n\")\n",
        "    seg = [\n",
        "        list(map(float, c.split(\" \")[2:4]))\n",
        "        + [c.split(\" \")[4].replace(\"<b>\", \"^\"), re.sub(rf\"[^{WORD_CHARS}]+\", \"\", c.split(\" \")[4].replace(\"<b>\", \"\"))]\n",
        "        for c in ctm_lines\n",
        "        if c\n",
        "    ]\n",
        "start_idx = 0\n",
        "line_iter = iter([line for line in lines if line[\"match\"]])\n",
        "try:\n",
        "    current_line = next(line_iter)\n",
        "    match = current_line[\"match\"]\n",
        "    for idx, segment in enumerate(seg):\n",
        "        if start_idx <= idx + 2:\n",
        "            match = match.lstrip(segment[3])\n",
        "            if match == \"\":\n",
        "                start_idx = next(i for i, s in enumerate(seg) if i >= start_idx and s[3])\n",
        "                start = seg[start_idx][0] * 1000\n",
        "                if idx + 1 < len(seg):\n",
        "                    end = (seg[idx + 1][0] + seg[idx + 1][1]) * 1000\n",
        "                else:\n",
        "                    end = (seg[idx][0] + seg[idx][1]) * 1000\n",
        "                current_line[\"debug_ass\"] = pysubs2.SSAEvent(\n",
        "                    start=start,\n",
        "                    end=end,\n",
        "                    text=\"\".join([f\"{{\\\\kf{round(seg_item[1] * 100)}}}{seg_item[2]}\" for seg_item in seg[start_idx : idx + 2]]),\n",
        "                    name=current_line[\"name\"] if \"name\" in current_line else \"\",\n",
        "                )\n",
        "                long_seg = [\n",
        "                    [(i / (idx + 1 - start_idx)) ** 1.5, s[1] - 0.8]\n",
        "                    for i, s in enumerate(seg[start_idx : idx + 2])\n",
        "                    if s[1] > 0.8\n",
        "                ]\n",
        "                if long_seg:\n",
        "                    avg_position = sum(x[0] * (x[1] ** 2) for x in long_seg) / sum(x[1] ** 2 for x in long_seg)\n",
        "                    total_delta = sum(x[1] for x in long_seg) * 1000\n",
        "                    start = start + (1 - avg_position) * total_delta\n",
        "                    end = end - avg_position * total_delta\n",
        "                current_line[\"res_ass\"] = pysubs2.SSAEvent(\n",
        "                    start=start, end=end, text=current_line[\"raw\"], name=current_line[\"name\"] if \"name\" in current_line else \"\"\n",
        "                )\n",
        "                start_idx = idx + 2\n",
        "                current_line = next(line_iter)\n",
        "                match = current_line[\"match\"]\n",
        "except StopIteration:\n",
        "    pass\n",
        "subs = pysubs2.SSAFile()\n",
        "subs.styles[\"Default\"] = pysubs2.SSAStyle(\n",
        "    fontname=\"IPAexGothic\",\n",
        "    fontsize=40,\n",
        "    primarycolor=pysubs2.Color(r=255, g=255, b=255, a=0),\n",
        "    secondarycolor=pysubs2.Color(r=0, g=213, b=255, a=0),\n",
        "    outlinecolor=pysubs2.Color(r=0, g=0, b=0, a=0),\n",
        "    backcolor=pysubs2.Color(r=74, g=74, b=74, a=0),\n",
        "    bold=True,\n",
        "    alignment=pysubs2.Alignment.BOTTOM_CENTER,\n",
        "    shadow=0,\n",
        ")\n",
        "subs.styles[\"Top\"] = subs.styles[\"Default\"].copy()\n",
        "subs.styles[\"Top\"].alignment = pysubs2.Alignment.TOP_CENTER\n",
        "subs.info[\"PlayResX\"] = \"1920\"\n",
        "subs.info[\"PlayResY\"] = \"1080\"\n",
        "subs.events = [l[\"debug_ass\"] for l in lines if l[\"processed\"]]\n",
        "subs.save(f\"{WORK_DIR}/debug.ass\")\n",
        "subs.events = []\n",
        "for idx, l in enumerate(lines):\n",
        "    if l[\"processed\"]:\n",
        "        subs.events.append(l[\"res_ass\"])\n",
        "    else:\n",
        "        prev_event = None\n",
        "        next_event = None\n",
        "        prev_event = next((lines[i][\"res_ass\"] for i in range(idx - 1, -1, -1) if lines[i][\"processed\"]), None)\n",
        "        next_event = next((lines[i][\"res_ass\"] for i in range(idx + 1, len(lines)) if lines[i][\"processed\"]), None)\n",
        "        if prev_event and next_event:\n",
        "            x = (prev_event.end + next_event.start) / 2.0\n",
        "            start = x - 500\n",
        "            end = x + 500\n",
        "        elif next_event:\n",
        "            start = next_event.start - 1000\n",
        "            end = next_event.start\n",
        "        elif prev_event:\n",
        "            start = prev_event.end\n",
        "            end = prev_event.end + 1000\n",
        "        else:\n",
        "            start = 0\n",
        "            end = 1000\n",
        "        subs.events.append(pysubs2.SSAEvent(start=start, end=end, text=l[\"raw\"], style=\"Top\"))\n",
        "subs.save(\"fa.ass\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LZKXogkeImj"
      },
      "source": [
        "### VAD-assisted refinement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GficIUWEeLMo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pysubs2\n",
        "\n",
        "SUBS = r\"fa.ass\"\n",
        "VAD = r\"vad.srt\"\n",
        "\n",
        "# all in milliseconds\n",
        "THRESHOLD_GAP = 40  # min gap before/after a line to be considered a true gap (for start_with_gap/end_with_gap)\n",
        "THRESHOLD_SAME_TIME = 40  # min difference between two timestamps to be considered different\n",
        "WINDOW_START_WITH_GAP = [-1800, 1800]\n",
        "WINDOW_END_WITH_GAP = [-1800, 1800]\n",
        "WINDOW_ADJOINT = [-800, 800]\n",
        "\n",
        "\n",
        "def IS_DIALOGUE(e):\n",
        "    return e.style == \"Default\"\n",
        "\n",
        "\n",
        "def has_overlap(start1, end1, start2, end2):\n",
        "    return end1 >= start2 and end2 >= start1\n",
        "\n",
        "\n",
        "subs = pysubs2.load(SUBS)\n",
        "subs.sort()\n",
        "subs_dialog = [e for e in subs.events if IS_DIALOGUE(e)]\n",
        "subs_other = [e for e in subs.events if not IS_DIALOGUE(e)]\n",
        "\n",
        "vad = pysubs2.load(VAD)\n",
        "vad.sort()\n",
        "\n",
        "# THRESHOLD_GAP before start does not overlap any other event\n",
        "start_with_gap = [\n",
        "    (i, e.start)\n",
        "    for i, e in enumerate(subs_dialog)\n",
        "    if not any(\n",
        "        j != i and has_overlap(subs_dialog[j].start, subs_dialog[j].end, e.start - THRESHOLD_GAP, e.start)\n",
        "        for j in range(len(subs_dialog))\n",
        "    )\n",
        "]\n",
        "count = 0\n",
        "for i, this_start in start_with_gap:\n",
        "    candidates = [\n",
        "        v.start for v in vad.events if this_start + WINDOW_START_WITH_GAP[0] < v.start < (this_start + WINDOW_START_WITH_GAP[1])\n",
        "    ]\n",
        "    if candidates:\n",
        "        match = min(candidates, key=lambda x: abs(x - this_start))\n",
        "        subs_dialog[i].start = match\n",
        "        count += 1\n",
        "print(f\"start_with_gap: adjusted {count}/{len(start_with_gap)} boundaries\")\n",
        "\n",
        "\n",
        "# THRESHOLD_GAP after end does not overlap any other event\n",
        "end_with_gap = [\n",
        "    (i, e.end)\n",
        "    for i, e in enumerate(subs_dialog)\n",
        "    if not any(\n",
        "        j != i and has_overlap(subs_dialog[j].start, subs_dialog[j].end, e.end, e.end + THRESHOLD_GAP)\n",
        "        for j in range(len(subs_dialog))\n",
        "    )\n",
        "]\n",
        "count = 0\n",
        "for i, this_end in end_with_gap:\n",
        "    candidates = [\n",
        "        v.end for v in vad.events if (this_end + WINDOW_END_WITH_GAP[0]) < v.end < (this_end + WINDOW_END_WITH_GAP[1])\n",
        "    ]\n",
        "    if candidates:\n",
        "        match = min(candidates, key=lambda x: abs(x - this_end))\n",
        "        subs_dialog[i].end = match\n",
        "        count += 1\n",
        "print(f\"end_with_gap: adjusted {count}/{len(end_with_gap)} boundaries\")\n",
        "\n",
        "\n",
        "# event i ends just at event j's start\n",
        "adjoint = [\n",
        "    (i, j, e_i.end)\n",
        "    for i, e_i in enumerate(subs_dialog)\n",
        "    for j, e_j in enumerate(subs_dialog)\n",
        "    if i != j and abs(e_i.end - e_j.start) < THRESHOLD_SAME_TIME\n",
        "]\n",
        "vad_gaps = [(vad.events[i].end, vad.events[i + 1].start) for i in range(len(vad.events) - 1)]\n",
        "count = 0\n",
        "for i, j, this_boundary in adjoint:\n",
        "    max_gap_duration = 0\n",
        "    nearest_gap = None\n",
        "    min_distance = float(\"inf\")\n",
        "    for gap_start, gap_end in vad_gaps:\n",
        "        if has_overlap(gap_start, gap_end, this_boundary + WINDOW_ADJOINT[0], this_boundary + WINDOW_ADJOINT[1]):\n",
        "            distance = abs(gap_start - this_boundary)\n",
        "            if distance < min_distance:\n",
        "                min_distance = distance\n",
        "                nearest_gap = (gap_start, gap_end)\n",
        "    if nearest_gap:\n",
        "        subs_dialog[i].end = nearest_gap[0]\n",
        "        subs_dialog[j].start = nearest_gap[1]\n",
        "        max_gap_duration = nearest_gap[1] - nearest_gap[0]\n",
        "    if max_gap_duration > 0:\n",
        "        count += 1\n",
        "print(f\"adjoint: adjusted {count}/{len(adjoint)} boundaries\")\n",
        "\n",
        "for i, e in enumerate(subs_dialog):\n",
        "    if i - 1 >= 0 and abs(e.start - subs_dialog[i - 1].start) < THRESHOLD_SAME_TIME:\n",
        "        e.start = subs_dialog[i - 1].end\n",
        "    if i - 1 >= 0 and abs(e.end - subs_dialog[i - 1].end) < THRESHOLD_SAME_TIME:\n",
        "        subs_dialog[i - 1].end = e.start\n",
        "for e in subs_dialog:\n",
        "    if e.start >= e.end:\n",
        "        e.effect = \"!\"\n",
        "        e.start, e.end = e.end, e.start\n",
        "for e in subs_dialog:\n",
        "    if e.duration <= 400:\n",
        "        e.effect = \"!\"\n",
        "        e.start -= 200\n",
        "        e.end += 200\n",
        "subs.events = subs_dialog + subs_other\n",
        "\n",
        "subs.save(os.path.join(os.path.dirname(SUBS), f\"{'.'.join(os.path.basename(SUBS).split('.')[:-1])}+vad.ass\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFVy4j9Ojg9T"
      },
      "source": [
        "### Download the result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXB3VmIzjfsl"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download(\"/content/fa+vad.ass\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "B_D9PXJakAZA",
        "b21WkFmcejZj",
        "Mc52ZpjPbCEj",
        "BgnaZ9udbwJh",
        "-LZKXogkeImj",
        "XFVy4j9Ojg9T"
      ],
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
